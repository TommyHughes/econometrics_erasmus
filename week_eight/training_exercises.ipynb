{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 Training Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Exercise M1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Given the model and values, calculate the values of the vector $e$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e1:  [-0.61 -4.06 -9.12 -1.19 -0.13]\n",
      "e2:  [ 6.81  7.29 -0.61 -0.08  2.4 ]\n"
     ]
    }
   ],
   "source": [
    "# Create the data for the model y = Xb + e\n",
    "\n",
    "y = np.array([15.1, 7.9, 4.5, 12.8, 10.5])\n",
    "\n",
    "X = np.array(\n",
    "    [[1,25.5,1.23]\n",
    "     ,[1,40.8,1.89]\n",
    "     ,[1,30.2,1.55]\n",
    "     ,[1,4.3,1.18]\n",
    "     ,[1,10.7,1.68]]\n",
    ")\n",
    "\n",
    "b_1 = np.array([23, 0.1, -8])\n",
    "b_2 = np.array([22, -0.2, -7])\n",
    "\n",
    "# Calculate e\n",
    "e_1 = y - np.matmul(X, b_1)\n",
    "e_2 = y - np.matmul(X, b_2)\n",
    "\n",
    "# Print results\n",
    "print(\"e1: \", e_1)\n",
    "print(\"e2: \", e_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Which $b$ gives the smallest $e$ by both absolute value and by squaring?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't observe a strict order on the resultant $e$ vectors, since the calculations are done component-wise. However, if we aggregate the resultant $e$ vectors by summing them component-wise, then with respect to both measures, $b_1$ appears to produce the smallest $e$.\n",
    "\n",
    "Detailed calculations are included below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Absolute --------\n",
      "e1\n",
      "\tabs: [0.61 4.06 9.12 1.19 0.13]\n",
      "\tsum: 15.11\n",
      "e2\n",
      "\tabs: [6.81 7.29 0.61 0.08 2.4 ]\n",
      "\tsum: 17.19\n",
      "-------- Squared --------\n",
      "e1\n",
      "\tabs: [3.72100e-01 1.64836e+01 8.31744e+01 1.41610e+00 1.69000e-02]\n",
      "\tsum: 101.46309999999998\n",
      "e2\n",
      "\tabs: [4.63761e+01 5.31441e+01 3.72100e-01 6.40000e-03 5.76000e+00]\n",
      "\tsum: 105.65870000000002\n"
     ]
    }
   ],
   "source": [
    "# Calculate e sizes\n",
    "e_1_abs = np.abs(e_1)\n",
    "e_2_abs = np.abs(e_2)\n",
    "\n",
    "e_1_sq = np.square(e_1)\n",
    "e_2_sq = np.square(e_2)\n",
    "\n",
    "# Print results\n",
    "print(\"-------- \"+\"Absolute\"+\" --------\")\n",
    "print(\"e1\")\n",
    "print(\"\\tabs:\", e_1_abs)\n",
    "print(\"\\tsum:\", np.sum(e_1_abs))\n",
    "print(\"e2\")\n",
    "print(\"\\tabs:\", e_2_abs)\n",
    "print(\"\\tsum:\", np.sum(e_2_abs))\n",
    "\n",
    "print(\"-------- \"+\"Squared\"+\" --------\")\n",
    "print(\"e1\")\n",
    "print(\"\\tabs:\", e_1_sq)\n",
    "print(\"\\tsum:\", np.sum(e_1_sq))\n",
    "print(\"e2\")\n",
    "print(\"\\tabs:\", e_2_sq)\n",
    "print(\"\\tsum:\", np.sum(e_2_sq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Find the dimension of d and write it in sigma notation $\\left(\\Sigma\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since\n",
    "\n",
    "$$ d = u_{1 \\times p} \\cdot A_{p \\times q} \\cdot v_{q \\times 1} $$\n",
    "\n",
    "it follows $d$ is $(1 \\times 1)$ (a scalar). We can compute $d$ by\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    d &= \\sum_{r=1}^p u_{ir} \\left[ Av \\right]_{r1} \\\\\n",
    "    &= \\sum_{r=1}^{p} u_{ir} \\left( \\Sigma_{s=1}^q a_{rs} v_{s1} \\right) \\\\\n",
    "    &= \\sum_{r=1}^{p} \\sum_{s=1}^{q} u_{ir} a_{rs} v_{s1}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Simplify $\\left( A + I \\right)^2$ with $A$ $(p \\times p)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "$$\n",
    "    (A + I)^2 = A^2 + 2A + I\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Exercise M2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Find $(a+b)^T (a+b)$ for $a,b$ $(p \\times 1)$ vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    (a+b)^T (a+b) &= (b^T + a^T) (a+b) \\\\\n",
    "    &= b^Ta + b^Tb + a^Ta + a^Tb \\\\\n",
    "    &= a^Ta + 2a^Tb + b^Tb\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Let $a$ be $(p \\times 1)$. Show $\\text{tr}(a^Ta) = \\text{tr}(aa^T)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** Observe that\n",
    "\n",
    "$$ \\left[ a a^T \\right]_{ii} = a_{i1} [a^T]_{1i} = a_{i}^2 $$\n",
    "\n",
    "which immediately implies the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Let $A$ be square $(p \\times p)$ and $c$ a scalar. Show $\\text{tr}(cA) = c \\text{tr}(A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{tr}(cA) &= \\sum_{i=1}^p c a_{ii} \\\\\n",
    "    &= c \\sum_{i=1}^p a_{ii} \\\\\n",
    "    &= c \\text{tr}(A)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Let $A$ be square $(p \\times p)$ and invertible and $c \\neq 0$ a scalar.Find $(cA)^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** Observe\n",
    "\n",
    "$$ \\frac{1}{c}A^{-1} cA = \\frac{1}{c} c A^{-1}A = I $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Find the value of $f$ that makes $B$ the inverse of $A$ and state the condition the elements of $A$ must satisfy so that the inverse exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** If $f = ad - bc \\neq 0 $ then one can verify that $ AB = BA = I$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Simplify $\\iota^T \\iota$ and $(\\iota \\iota^T)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** Observe that\n",
    "\n",
    "$$ \\iota^T \\iota = \\sum_{i=1}^p (1 \\cdot 1) = p $$\n",
    "\n",
    "and that\n",
    "\n",
    "$$ (\\iota \\iota^T) = \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} \\begin{bmatrix} 1 &\\ldots & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & \\ldots & 1 \\\\ \\vdots & \\ddots & \\vdots \\\\ 1 &\\ldots & 1 \\end{bmatrix} $$\n",
    "\n",
    "so that\n",
    "\n",
    "$$ \\left[ (\\iota \\iota^T)^2 \\right]_{ij} = \\sum_{r=1}^p (\\iota \\iota^T)_{pr} (\\iota \\iota^T)_{rp} = \\sum_{r=1}^p 1 \\cdot 1 = p $$\n",
    "\n",
    "and therefore\n",
    "\n",
    "$$ (\\iota \\iota^T)^2 = \\begin{bmatrix} p & \\ldots & p \\\\ \\vdots & \\ddots & \\vdots \\\\ p & \\ldots & p \\end{bmatrix} = p \\iota \\iota^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Exercise M3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Find the gradient and Hessian of $f(b) = b^T b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** We have that\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial f}{\\partial b_i} (b) &= \\frac{\\partial}{\\partial b_i} \\left( \\sum_{r=1}^p b_r^2 \\right) \\\\\n",
    "    &= \\sum_{r=1}^p \\frac{\\partial}{\\partial b_i} b_r^2 \\\\\n",
    "    &= 2b_i\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial b} (b) = 2b $$\n",
    "\n",
    "For the Hessian, we have that\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial b_i \\partial b_j} (b) = \\frac{\\partial}{\\partial b_i} \\left( 2b_j \\right) $$\n",
    "\n",
    "so that\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial f}{\\partial b \\partial b^T} (b) = \\begin{cases} 2 & ,i=j \\\\ 0 & ,\\text{otherwise} \\end{cases} = 2I\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Prove a diagonal matrix is positive definite if all diagonal elements are positive, and negative definite if all diagonal elements are negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof:** Let $D$ be a $(p \\times p)$ diagonal matrix and let $x \\neq 0$ be a $(p \\times 1)$ vector. Then\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    x^T D x &= \\sum_{r=1}^p [x^T]_{1r} \\left[ Dx \\right]_{r1} \\\\\n",
    "    &= \\sum_{r=1}^p x_r d_{rr} x_r \\\\\n",
    "    &= \\sum_{r=1}^p d_{rr} x_r^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "so that, since $x \\neq 0$, $x^T D x$ is positive definite when $d_{rr} > 0$ for all $1 \\leq r \\leq p$ and is negative definite when $d_{rr} < 0$ for all $1 \\leq r \\leq p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Given $y = Xb + e$ with the stated assumptions, find the vector $b^*$ that minimizes $f(b) = e^T e$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** Since $e = e(b) = y - Xb$ it follows that\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    f(b) &= \\left( y - Xb \\right)^T \\left( y - Xb \\right) \\\\\n",
    "    &= y^Ty - 2y^TXb + b^TX^TXb\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now, since $\\frac{\\partial x^TAx}{\\partial x} (x) = \\left( A + A^T \\right) x$ it follows that\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial f}{\\partial b} (b)  = -2X^Ty + 2X^TXb\n",
    "$$\n",
    "\n",
    "so that, by invertibility of $X^TX$ which follows from $X$ having full column-rank, solving $\\frac{\\partial f}{\\partial b} (b^*) = 0$ yields\n",
    "\n",
    "$$ b^* =  \\left( X^TX \\right)^{-1} y^TX $$\n",
    "\n",
    "so that\n",
    "\n",
    "$$ H(b^*) = 2X^TX $$\n",
    "\n",
    "and since $X$ has full column-rank, $Xv = 0$ if and only if $v = 0$. Thus\n",
    "\n",
    "$$ v^T X^TX v = (Xv)^T (Xv) > 0 $$\n",
    "\n",
    "since $v \\neq 0$ implies $Xv \\neq 0$. Thus $b^*$ minimizes $f$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Exercise P1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Find the mean and variance of $y = a + b^Tx + e$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** Since $y$ is an affine transformation of the random variable $e$, it follows\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mu_y &= a + b^Tx + \\mu_e \\\\\n",
    "    \\sigma^2_y &= \\sigma_e^2\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Find the mean and variance of $y = a_1x_1 - a_2x_2$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** We have that\n",
    "\n",
    "$$\n",
    "    \\mu_y = E[a_1x_1 - a_2x_2] = a_1 \\mu_{x_1} - a_2 \\mu_{x_2}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\sigma_y^2 &= E[( a_1(x_1 - \\mu_{x_1}) - a_2(x_2 - \\mu_{x_2}) )^2] \\\\\n",
    "    &= a_1^2 E[(x_1 - \\mu_{x_1})^2] + a_2^2 E[(x_2 - \\mu_{x_2})^2] - 2a_1a_2 E[(x_1 - \\mu_{x_1})(x_2 - \\mu_{x_2})] \\\\\n",
    "    &= a_1^2 \\sigma_{x_1}^2 + a_2^2 \\sigma_{x_2}^2 - 2a_1a_2 \\sigma_{x_1 x_2}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Find the mean and variance of $y = a_1x_1 + (1-a_1)x_2$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** Similar to exercise 2,\n",
    "\n",
    "$$\n",
    "    \\mu_y = a_1 \\mu + (1-a_1) \\mu\n",
    "$$\n",
    "\n",
    "Now, observe that\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\rho &= \\frac{\\sigma_{x_1 x_2}}{\\sigma_{x_1} \\sigma_{x_2}} \\\\\n",
    "    \\sigma_{x_1} \\sigma_{x_2} \\rho &= \\sigma_{x_1 x_2} \\\\\n",
    "    \\sigma^2 \\rho &= \\sigma_{x_1 x_2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and thus, repeating a similar calculation as in 2, we get\n",
    "\n",
    "$$\n",
    "    \\sigma_y^2 = a_1^2 \\sigma^2 + (1 - a_1)^2 \\sigma^2 + 2a_1(1-a_1) \\sigma^2 \\rho\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Let $y = \\frac{1}{n} \\sum_{i=1}^n x_i$ where $x_i$ all have same mean and variance.\n",
    "\n",
    "a) Find mean and variance of $y$\n",
    "\n",
    "b)Find correlation of $y$ with $x_i$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "a) \n",
    "\n",
    "For the mean, we get\n",
    "\n",
    "$$\n",
    "    \\mu_y = E\\left[ \\frac{1}{n} \\sum_{i=1}^n x_i \\right] = \\frac{1}{n} \\sum_{i=1}^n E[x_i] = \\frac{1}{n} n \\mu = \\mu\n",
    "$$\n",
    "\n",
    "For the variance, recall that $\\text{var}[y] = a^T \\Sigma a$ which, in this case, we have $a = \\frac{1}{n} \\iota$ and, since covariances are all zero, $\\Sigma$ is a diagonal matrix with $\\sigma^2$ along the diagonal. Thus\n",
    "\n",
    "$$\n",
    "    \\sigma_y^2 = a^T \\Sigma a = \\frac{1}{n} \\iota^T \\Sigma \\frac{1}{n} \\iota = \\frac{1}{n^2} \\begin{bmatrix} 1 \\ldots 1 \\end{bmatrix} \\begin{bmatrix} \\sigma^2 & & 0 \\\\ & \\ddots & \\\\ 0 & & \\sigma^2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} = \\underbrace{\\frac{\\sigma^2}{n^2} + \\ldots + \\frac{\\sigma^2}{n^2}}_{n-times} = \\frac{\\sigma^2}{n}\n",
    "$$\n",
    "\n",
    "b) Recall that\n",
    "\n",
    "$$\n",
    " \\rho = \\frac{\\text{cov}(x_i, y)}{\\sigma_{x_i} \\sigma_y} = \\frac{\\text{cov}(x_i, y)}{ \\frac{\\sigma^2}{\\sqrt{n}} }\n",
    "$$\n",
    "\n",
    "Now, it is easy to show that $E[x_i^2] = \\sigma^2 + \\mu^2$ and $E[x_i x_j] = \\mu^2$ when $i \\neq j$. Thus it follows that\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{cov}(x_i, y) &= E[(x_i - \\mu)(y - \\mu)] \\\\\n",
    "    &= E[x_i y] - \\mu^2 \\\\\n",
    "    &= \\frac{1}{n} \\sum_{j=1}^n E[x_i x_j] - \\mu^2 \\\\\n",
    "    &= \\frac{1}{n} \\left( E[x_i^2] + \\sum_{j=1, j \\neq i}^n E[x_i x_j] \\right) - \\mu^2 \\\\\n",
    "    &= \\frac{1}{n} \\left( \\sigma^2 + \\mu^2 + (n-1) \\mu^2 \\right) - \\mu^2 \\\\\n",
    "    &= \\frac{1}{n} \\sigma^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and therefore\n",
    "\n",
    "$$\n",
    "    \\rho = \\frac{\\text{cov}(x_i, y)}{ \\frac{\\sigma^2}{\\sqrt{n}} } = \\frac{ \\frac{1}{n} \\sigma^2 }{ \\frac{\\sigma^2}{\\sqrt{n}} } = \\frac{1}{\\sqrt{n}}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Let $h = \\frac{1}{n} \\iota$ and $ H = \\begin{bmatrix} I \\\\ h^t \\end{bmatrix}_{(n+1) \\times n}$ and $x_{n \\times 1}$ be a vector of random variables all with same mean and variance and independent.\n",
    "\n",
    "a) Derive the mean and variance of $y = h^t x$\n",
    "\n",
    "b) Derive the meana and variance of $z = Hx$\n",
    "\n",
    "c) Compare your answers to the previous question."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "a) Observe that\n",
    "\n",
    "$$\n",
    "    y = h^T x = \\frac{1}{n} \\iota^T x = \\frac{1}{n} \\sum_{i=1}^n x_i\n",
    "$$\n",
    "\n",
    "so that the mean and variance are the same as in part (a) of the previous question ($\\mu$ and $\\sigma^2/n$ respectively).\n",
    "\n",
    "b) Observe that\n",
    "\n",
    "$$\n",
    "    z = Hx = \\begin{bmatrix} x \\\\ y \\end{bmatrix}_{(n+1) \\times 1}\n",
    "$$\n",
    "\n",
    "so that, since the mean of $y$ is $\\mu$ the mean vector of $z$ is $\\mu \\iota_{n+1}$. Since the $x_i$ are independent, it follows from the previous that $\\text{cov}(x_i, y) = \\frac{\\sigma^2}{n}$ and independence of $x_i\\text{'s}$ that\n",
    "\n",
    "$$\n",
    "    \\Sigma_z =\n",
    "        \\left[ \\begin{array}{c | c}\n",
    "            \\sigma^2 I_n & \\frac{\\sigma^2}{n} \\iota_n \\\\ \\hline\n",
    "            \\frac{\\sigma^2}{n} \\iota_n^T & \\frac{\\sigma^2}{n}\n",
    "        \\end{array} \\right]_{(n+1) \\times (n+1)}\n",
    "$$\n",
    "\n",
    "c) The previous answers were used to produce the solutions in this problem. This presents a matrix and vector approach to expressing the same ideas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Exercise P2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Transform the rule of thumb probabilities of a standard normal to a normal distribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** We have that for $y \\sim N(0,1)$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    P(-1 \\leq y \\leq 1) &\\approx 0.68 \\\\\n",
    "    P(-2 \\leq y \\leq 2) &\\approx 0.95\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "but then $\\frac{x-\\mu}{\\sigma} = y$ so that\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    -1 \\leq y \\leq 1 &\\iff \\mu-\\sigma \\leq x \\leq \\mu + \\sigma \\\\\n",
    "    -2 \\leq y \\leq 2 &\\iff \\mu - 2\\sigma \\leq x \\leq \\mu + 2\\sigma\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "so that\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    P(-1 \\leq y \\leq 1) = P(\\mu-\\sigma \\leq x \\leq \\mu + \\sigma ) &\\approx 0.68 \\\\\n",
    "    P(-2 \\leq y \\leq 2) = P(\\mu - 2\\sigma \\leq x \\leq \\mu + 2\\sigma) &\\approx 0.95\n",
    "\\end{align*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Derive the variance of a chi-square with $n$ degrees of freedom."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** So $z = \\sum_{i=1}^n x_i^2$ where $x_i \\sim \\text{NID}(0,1)$. Recall that $E[z] = n$. Also, recall that, for a standard normal variable, we have\n",
    "\n",
    "$$\n",
    "    E[x_i^4] = \\text{kurtosis} = 3\n",
    "$$\n",
    "\n",
    "Note also that\n",
    "\n",
    "$$\n",
    "    E[x_i^2 x_j^2] = E[x_i^2] E[x_j^2] = 1 \\cdot 1 = 1\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{var}(z) &= E[z^2] - n^2 \\\\\n",
    "    &= E\\left[ \\left( \\sum_{i=1}^n x_i^2 \\right)^2 \\right] - n^2 \\\\\n",
    "    &= \\sum_{i=1}^n E[x_i^4] + \\sum_{i,j=1, i\\neq j}^n E[x_i^2 x_j^2] - n^2 \\\\\n",
    "    &= (3n) + (n^2 - n) - n^2 \\\\\n",
    "    &= 3n -n \\\\\n",
    "    &= 2n\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Let $z_1, z_2$ be independent chi-square with $k_1, k_2$ degrees of freedom. Show $z_1 + z_2 \\sim \\chi^2(k_1+k_2)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** Since everything is independent, we have\n",
    "\n",
    "$$\n",
    "    z_1 + z_2 = \\sum_{i=1}^{k_1} x_i^2 + \\sum_{i=k_1 + 1}^{k_1 + k_2} x_i^2 = \\sum_{i=1}^{k_1 + k_2} x_i^2\n",
    "$$\n",
    "\n",
    "where $x_i \\sim \\text{NID}(0,1)$. Thus $z_1 + z_2 \\sim \\chi^2(k_1 + k_2)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Let $x$ be an $n \\times 1$ random vector that follows a multivariate normal with mean vector $\\mu$ and covariance matrix $\\Sigma$. Let $L$ be an invertible matrix such that $LL^T = \\Sigma$.\n",
    "\n",
    "a) What is the distribution of $y = L^{-1}(x-\\mu)$\n",
    "\n",
    "b) Show $z = (x - \\mu)^T \\Sigma^{-1} (x - \\mu)$ follows $\\chi^2(n)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "a) Since $L^{-1}$ is a linear transformation in follows that\n",
    "\n",
    "$$\n",
    "    y = L^{-1}(x - \\mu) = L^{-1}x - L^{-1}\\mu\n",
    "$$\n",
    "\n",
    "Since $ L^{-1}\\mu - L^{-1}\\mu = 0_{n \\times 1}$ and \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    L^{-1} \\Sigma \\left( L^{-1} \\right)^T &= L^{-1} \\left( L L^T \\right) \\left( L^{-1} \\right)^T \\\\\n",
    "    &= L^T \\left( L^{-1} \\right)^T \\\\\n",
    "    &= L^T \\left( L^T \\right)^{-1} \\\\\n",
    "    &= I_n\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "by linearity, we have that $y \\sim N(0_{n \\times 1}, I_n)$\n",
    "\n",
    "b) Observe that\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    (x - \\mu)^T \\Sigma^{-1} (x - \\mu) &= (x - \\mu)^T (L L^T)^{-1} (x - \\mu) \\\\\n",
    "    &= (x - \\mu)^T (L^T)^{-1} L^{-1} (x - \\mu) \\\\\n",
    "    &= (x - \\mu)^T (L^{-1})^T L^{-1} (x - \\mu) \\\\\n",
    "    &= \\left( L^{-1} (x - \\mu) \\right)^T \\left( L^{-1} (x - \\mu) \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "which, by (a), we know implies we are talking the inner product of two n-dimensional vectors with standard normal variables, which is the sum of squares of $n$ standard normal random variables.\n",
    "\n",
    "Thus $ (x - \\mu)^T \\Sigma^{-1} (x - \\mu) \\sim \\chi^2(n)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Let $y_{n \\times 1} \\sim N(0,I_n)$. Let $z \\sim \\chi^2(k)$ independent from $y$. Given the formula for $x$, derive the distribution of $x^T x /n$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** Clearly $y^T y \\sim \\chi^2(n)$. Thus\n",
    "\n",
    "$$\n",
    "    \\frac{x^Tx}{n} = \\frac{1}{n} \\left( \\frac{1}{z/k} y^T \\frac{1}{z/k} y \\right) = \\frac{ \\frac{y^Ty}{n} }{ \\frac{z}{k} }\n",
    "$$\n",
    "\n",
    "so that $x^T x/n \\sim F(n,k)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b8fa3177836abb39d5e3b555ef652ac0e569b7e89a422512b518d324b09235a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
