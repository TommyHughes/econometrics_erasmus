{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Week 8 Lectures: Building Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M1: Introduction to Vectors and Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To denote the $i^{\\text{th}}$ row of matrix $A$ by\n",
    "\n",
    "$$A_{i \\bullet}$$\n",
    "\n",
    "Similarly, we denote the $j^{\\text{th}}$ column of matrix $A$ by\n",
    "\n",
    "$$A_{\\bullet j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $v$ is a $(p \\times 1)$ column-vector and $x$ is a $(1 \\times p)$ row-vector, then\n",
    "\n",
    "$$v \\cdot x = Y$$\n",
    "\n",
    "is called the **outer product** and $Y$ is a $(p \\times q)$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We denote a column-vector whose entries are all 1 by $\\iota$. That is\n",
    "\n",
    "$$\\iota = \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}$$\n",
    "\n",
    "For this course, we sometimes refer to this as **the unit vector** (note this is in contrast to the typical meaning of unit vector: $\\vert v \\vert = 1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M2: Special Matrix Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this course, the transpose of $A$ may be denoted $A'$ or by $A^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M3: Vectors and Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of a function $f(\\mathbf{b}) = f(b_1, \\ldots, b_q)$ is the column-vector\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial \\mathbf{b}} (\\mathbf{b}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial b_1} (\\mathbf{b}) \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial b_q} (\\mathbf{b}) \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Hessian** is given by\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial \\mathbf{b} \\partial \\mathbf{b}^T} (\\mathbf{b}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial b_1 \\partial b_1} (\\mathbf{b}) & \\ldots & \\frac{\\partial f}{\\partial b_1 \\partial b_q} (\\mathbf{b}) \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f}{\\partial b_q \\partial b_1} (\\mathbf{b}) & \\ldots & \\frac{\\partial f}{\\partial b_q \\partial b_q} (\\mathbf{b}) \\end{bmatrix} $$\n",
    "\n",
    "When $f$ is smooth the Hessian is symmetric."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1: Random Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When given $n$ random variables, there are $\\frac{n(n-1)}{2}$ covariance combinations of different random variables since\n",
    "\n",
    "$$ {n \\choose 2} = \\frac{n!}{2!(n-2)!} = \\frac{n(n-1)}{2} $$\n",
    "\n",
    "and, of course, $n$ variances."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We denote the covariance matrix by\n",
    "\n",
    "$$\n",
    "    \\Sigma =\n",
    "        \\begin{bmatrix}\n",
    "            \\sigma_{11}^2 & \\ldots & \\sigma_{1n}^2 \\\\\n",
    "            \\vdots & \\ddots & \\vdots \\\\\n",
    "            \\sigma_{n1}^2 & \\ldots & \\sigma_{nn}^2\n",
    "        \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2: Probability Distributions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $x$ and $y$ are independent, then\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    E[xy] &= E[x] E[y] \\\\\n",
    "    E[g(x) h(y)] &= E[g(x)] E[h(y)]\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $x$ is normally distributed with mean $\\mu$ and variance $\\sigma^2$ we write\n",
    "\n",
    "$$ x \\sim N(\\mu, \\sigma^2) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $x$ is an $n \\times 1$ vector of normally distributed random variables with mean vector $\\mu_{n \\times 1}$ and variance matrix $\\Sigma_{n \\times n}$ then we write\n",
    "\n",
    "$$ x \\sim N(\\mu, \\Sigma) $$\n",
    "\n",
    "where, in particular, $x_i \\sim N(\\mu_i, \\sigma_i^2)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $ y_{m \\times 1} = A_{m \\times n} x + b_{m \\times 1}$, then\n",
    "\n",
    "$$ y \\sim N(A\\mu+b, A \\Sigma A^T) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $y_i \\sim N(\\mu, \\sigma^2)$ for every $i$ then we say the $y_i$ are normally and identically distributed and denote this by NID. That is\n",
    "\n",
    "$$ y_i \\sim \\text{NID}(\\mu, \\sigma^2) $$\n",
    "\n",
    "When the distribution is not necessarily normal we write IID and say the variables are independent and identically distributed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $z = \\sum_{i=1}^n y_i^2$ where $y_i \\sim \\text{NID}(0,1)$ then $z \\sim \\chi^2(n)$. That is, $z$ is a chi-square distribution with $n$ degrees of freedom."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $y \\sim N(0,1)$ and $z \\sim \\chi^2(\\nu)$ with $y,z$ independent, then\n",
    "\n",
    "$$ \\frac{y}{ \\sqrt{ \\frac{z}{\\nu} } } \\sim t(\\nu) $$\n",
    "\n",
    "where $t(\\nu)$ is the Student's $t$ distribution with $\\nu$ degrees of freedom.\n",
    "\n",
    "Note that\n",
    "\n",
    "$$ \\lim_{\\nu \\rightarrow \\infty} t(\\nu) = N(0,1) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $z_1 \\sim \\chi^2(d_1)$ and $z_2 \\sim \\chi^2(d_2)$ be independent. Then\n",
    "\n",
    "$$ \\frac{z_1/d_1}{z_2/d_2} \\sim F(d_1,d_2) $$\n",
    "\n",
    "where $F(d_1,d_2)$ is an $F$-distribution with $(d_1,d_2)$ degrees of freedom."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S1: Parameter Estimation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this course, we must always start with an assumption about what kind of distribution observations come from, and that all observations are IID.\n",
    "\n",
    "For example, we might have a sample (observations) $\\left\\{ y_1, y_2, \\ldots , y_n \\right\\}$ and we would assume that $y_i \\sim NID(\\mu, \\sigma^2)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **statistic** is a function of random variables\n",
    "\n",
    "$$\n",
    "    g(y) = g(y_1, y_2, \\ldots , y_n)\n",
    "$$\n",
    "\n",
    "If $y_i$ are observations (which are random variables) $g$ would be a function of those observations. Thus a statistic is a random variable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **estimator** is a statistic related to a parameter (for example $\\mu$).\n",
    "\n",
    "In general we denote an estimator by $\\hat{\\theta}$ for parameter $\\theta$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $E[\\hat{\\theta}] = \\theta$ then we say $\\hat{\\theta}$ is unbiased. Otherwise we say it is biased and the bias is equal to $E[\\hat{\\theta}] - \\theta$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We say that $\\hat{\\theta}$ is an **efficient estimator** when $\\text{var}[\\hat{\\theta}]$ is lowest over a set of estimators."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given observations $\\left\\{ y_1, \\ldots, y_n \\right\\}$ (NID), if we define\n",
    "\n",
    "$$\n",
    "    m = \\frac{1}{n} \\sum_{i=1}^n y_i\n",
    "$$\n",
    "\n",
    "Then $E[m] = \\mu$ so that $m$ is an unbiased estimator of $\\mu$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the same observations, recall $\\sigma^2 = \\text{var}[y_i] = E[(y_i - \\mu)^2]$. We want to find an estimator of $\\sigma^2$. We do not know $\\mu$ but we can estimate with $m$. Thus, define\n",
    "\n",
    "$$\n",
    "    z_i = y_i - m\n",
    "$$\n",
    "\n",
    "It follows that $z$ is a linear transformation of $y$ since\n",
    "\n",
    "$$\n",
    "    z = y - \\iota m = y - \\iota \\cdot \\frac{1}{n} \\iota^T y = \\left( I_n - \\frac{1}{n} \\iota \\iota^T \\right) y = My\n",
    "$$\n",
    "\n",
    "It is not hard to show that\n",
    "\n",
    "- $M$ is symmetric: $M = M^T$\n",
    "- $\\text{tr}(M) = n-1$\n",
    "- $M$ is idempotent: $M^2 = M$\n",
    "\n",
    "which can be used to show that, by linearity\n",
    "\n",
    "$$\n",
    "    z \\sim N(0, \\sigma^2M)\n",
    "$$\n",
    "\n",
    "We want to find an unbiased estimator for $\\sigma^2$ using $z$. The derivation in these notes will differ from the lecture, which uses vector and matrix calculations. What we want to do is see if $E[\\Sigma_{i=1}^n z_i^2] = E[z^tz] = \\sigma^2$ and if it isn't, perhaps we can find a way to transform $z^T z$ to get an unbiased estimator. Keep in mind, we also want that this estimator is not a function of $\\mu$ since we don't know $\\mu$. To that end, observe that\n",
    "\n",
    "$$\n",
    "    \\sum_{i=1}^n (y_i - m)^2 = \\sum_{i=1}^n y_i^2 - nm^2\n",
    "$$\n",
    "\n",
    "From training exercise P1.4 (a) we know that $\\sigma_{m}^2 = \\frac{\\sigma^2}{n}$. Thus\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    E\\left[ \\sum_{i=1}^n z_i^2 \\right] &= E\\left[ \\sum_{i=1}^n (y_i - m)^2 \\right] \\\\\n",
    "    &= E\\left[ \\sum_{i=1}^n y_i^2 - nm^2 \\right] \\\\\n",
    "    &= \\sum_{i=1}^n E\\left[ y_i^2 \\right] - nE\\left[ m^2 \\right] \\\\\n",
    "    &= \\sum_{i=1}^n \\sigma^2 + \\mu^2 - n \\left( \\frac{\\sigma^2}{n} + \\mu^2 \\right)\\\\\n",
    "    &= n \\left( \\sigma^2 + \\mu^2  \\right) - n \\left( \\frac{\\sigma^2}{n} + \\mu^2 \\right) \\\\\n",
    "    &= n \\sigma^2 + n \\mu^2 - \\sigma^2 - n \\mu^2 \\\\\n",
    "    &= (n-1) \\sigma^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This implies then that\n",
    "\n",
    "$$\n",
    "    E\\left[ \\frac{1}{n-1} z^T z \\right] = \\sigma^2\n",
    "$$\n",
    "\n",
    "so that $\\frac{1}{n-1} z^T z$ is an unbiased estimator of $\\sigma^2$. Thus we define\n",
    "\n",
    "$$\n",
    "    s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (y_i - m)^2\n",
    "$$\n",
    "\n",
    "as our estimator for $\\sigma^2$ and it is unbiased:\n",
    "\n",
    "$$\n",
    "    E[s^2] = \\sigma^2\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be shown that\n",
    "\n",
    "$$\n",
    "    \\frac{z^T z}{\\sigma^2} \\sim \\chi^2(n-1)\n",
    "$$\n",
    "\n",
    "and $m$ and $s^2$ are independent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We say that an estimator $\\hat{\\theta}$ is **consistent** when\n",
    "\n",
    "$$\n",
    "    \\lim_{n \\rightarrow \\infty} E[\\hat{\\theta}] = \\theta \\hspace{1mm} \\text{ and } \\hspace{1mm} \\lim_{n \\rightarrow \\infty} \\text{var}[\\hat{\\theta}] = 0\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be shown that both $m$ and $s^2$ are consistent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b8fa3177836abb39d5e3b555ef652ac0e569b7e89a422512b518d324b09235a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
